{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4ac1f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pymupdf #library for extracting text from PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9bf60c",
   "metadata": {},
   "source": [
    "# Step 1:\n",
    "Implement word-based or subwordy tokenization.\n",
    "- remove special characters and tags from the documents\n",
    "- identify and justify the list of stop-words and remove them\n",
    "__\n",
    "1. `pymupdf.page.get_text(flags=0)` `flags=0` were used to deal with problems of the identification \"fi\", \"ff\" subwords  \n",
    "2. assuming all texts are in the directory as the file\n",
    "3. stop words were taken from https://countwordsfree.com/stopwords and match with the existed words in the texts and adding some more which appeared due to math formulas in the text\n",
    "4. all justification to regex in the code block \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "65af29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = pymupdf.open('text1.pdf')\n",
    "text1 = ''\n",
    "for page in doc1:\n",
    "    text1 += page.get_text(flags=0)\n",
    "\n",
    "doc2 = pymupdf.open('text2.pdf')\n",
    "text2 = ''\n",
    "for page in doc2:\n",
    "    text2 += page.get_text(flags=0)\n",
    "\n",
    "STOP_WORDS = ['page', 'obtain', 'further', 'four', 'are', 'when', 'that', 'how', 'due', 'been', 'indicate', \n",
    "              'up', 'over', 'least', 'therefore', 'also', 'novel', 'became', 'to', 'after', 'third', 'allow', \n",
    "              'shows', 'useful', 'while', 'way', 'there', 'detail', 'or', 'inner', 'keys', 'amount', 'and', \n",
    "              'where', 'usually', 'here', 'related', 'present', 'no', 'results', 'significantly', 'will', 'just', \n",
    "              'recent', 'seen', 'become', 'example', 'corresponding', 'into', 'significant', 'took', 'thereafter', \n",
    "              'different', 'using', 'despite', 'backward', 'former', 'appropriate', 'thus', 'information', 'within', \n",
    "              'it', 'able', 'but', 'less', 'containing', 'have', 'hence', 'noted', 'recently', 'few', 'those', 'having', \n",
    "              'was', 'end', 'following', 'let', 'who', 'them', 'states', 'latter', 'little', 'an', 'possible', 'clearly', \n",
    "              'quickly', 'appear', 'empty', 'help', 'gives', 'on', 'being', 'forward', 'side', 'important', 'since', \n",
    "              'value', 'follows', 'overall', 'followed', 'before', 'effect', 'especially', 'particular', 'rd', 'in', \n",
    "              'these', 'lower', 'known', 'part', 'successfully', 'during', 'per', 'although', 'itself', 'our', 'allows', \n",
    "              'last', 'research', 'several', 'more', 'very', 'this', 'ones', 'whereas', 'five', 'did', 'has', 'approximately', \n",
    "              'again', 'than', 'so', 'eight', 'of', 'around', 'uses', 'otherwise', 'through', 'along', 'respectively', 'million', \n",
    "              'describe', 'resulted', 'another', 'similar', 'some', 'because', 'low', 'full', 'all', 'by', 'et', 'al', 'contains', \n",
    "              'described', 'does', 'section', 'could', 'two', 'entirely', 'mean', 'throughout', 'available', 'well', \n",
    "              'three', 'makes', 'line', 'whole', 'not', 'can', 'whose', 'get', 'find', 'doing', 'why', 'amongst', 'each', \n",
    "              'affecting', 'given', 'used', 'opposite', 'already', 'seems', 'de', 'tried', 'cannot', 'need', 'bottom', \n",
    "              'as', 'relatively', 'thanks', 'instead', 'becomes', 'previously', 'were', 'consider', 'off', 'with', \n",
    "              'much', 'we', 'is', 'at', 'come', 'give', 'obtained', 'above', 'most', 'same', 'both', 'see', 'too', \n",
    "              'similarly', 'once', 'made', 'considering', 'would', 'various', 'us', 'then', 'ours', 'call', 'best',\n",
    "              'out', 'only', 'ca', 'if', 'nearly', 'right', 'somewhat', 'unlike', 'any', 'what', 'many', 'they',\n",
    "              'according', 'yet', 'must', 'first', 'their', 'other', 'together', 'however', 'twelve', 'except', 'slightly',\n",
    "              'between', 'needs', 'about', 'added', 'take', 'better', 'every', 'means', 'be', 'importance', 'course', 'even',\n",
    "              'resulting', 'next', 'words', 'across', 'second', 'inside', 'use', 'often', 'which', 'for', 'should', 'such', 'may',\n",
    "              'show', 'new', 'do', 'one', 'back', 'from', 'shown', 'sometimes', 'make', 'run', 'found', 'state', 'its', 'the',\n",
    "              'without', 'furthermore', 'i.e.', 'e.g.', 'wt', 'wo', 'wi', 'vw', 'ch', 'pn', 'wj', 'hs', 'ho', 'vwi', 'ewi', 'san', \n",
    "              'chi', 'nam', 'st','ht','xn','zn','ym','dk','dv','qw','kw','xw','pe','xi','zi','rl','ls','el', '...', 'qkt', 'ffn', 'dff',\n",
    "              'pos', 'moe', 'ppl', 'zhu', 'nal']\n",
    "\n",
    "def word_based_tokenization(text:str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"arxiv:.*\\n\", repl='', string=text) #deleting watermark of arxiv\n",
    "    text = re.sub(r'^.*?\\babstract\\b', repl='abstract', string=text, flags=re.DOTALL) #clear all text before abstract due to irrelevance\n",
    "    text = re.sub(r'\\breferences\\b\\n.*', repl='references', string=text, flags=re.DOTALL) #clear all text in references due to irrelevance\n",
    "    \n",
    "    \n",
    "    text = re.sub(r\"\\(\\w+\\)\", repl='', string=text) #remove from the text words in the parenthesis (because it's  very probable that it will be math symbols)\n",
    "\n",
    "    text = re.sub(r\"(\\d+\\.)|(\\.\\d+)\", repl='', string=text) #removing the dot which is next to digits\n",
    "    text = re.sub(r\"\\.\\n\", repl='. ', string=text) #replacing the \".\" at the end of the para with \". \"\n",
    "    text = re.sub(r\"\\. \", repl=' ', string=text) #replacing the \". \" at the end of the sentence with \" \"\n",
    "    \n",
    "    text = re.sub(r\"(\\d+-)|(-\\d+)\", repl='', string=text) #removing the dash which is next to digits\n",
    "    text = re.sub(r\"( +-)|(- +)\", repl='', string=text) #removing the dash which is next to space\n",
    "    text = re.sub(r\"(skip-\\ngram)\", repl='skip-gram', string=text) #dealing with the names transposition\n",
    "    text = re.sub(r\"(high-\\nquality)\", repl='high-quality', string=text) #dealing with the names transposition\n",
    "    text = re.sub(r\"([-][\\n])\", repl = '', string=text) #dealing with basic transposition\n",
    "    \n",
    "    pattern = r\"([a-z\\-@\\.]+)\" #extracting all words from the preprocessed text\n",
    "    words = re.findall(pattern, text)\n",
    "    \n",
    "    tokens = [word for word in words if len(word) > 1 and word  not in STOP_WORDS] #excluding words with length = 1\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7751dc95",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "Implementing the Bag-of-Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b10505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def bag_of_words(corpus:list):\n",
    "    words = dict()\n",
    "    min_freq = 2\n",
    "    \n",
    "    #creating words dictionary of all words found in the corpus \n",
    "    #with their frequencies according to text where they were found\n",
    "    #ex: {\"word\":[total, [in_text1, in_text2, ...]]}\n",
    "    for text in range(len(corpus)):\n",
    "        for word in corpus[text]:\n",
    "            if words.get(word) == None:\n",
    "                maps = [0 for  _ in range(len(corpus))]\n",
    "                maps[text] += 1\n",
    "                words[word] = [1, maps]\n",
    "            else:\n",
    "                words[word][0] += 1\n",
    "                words[word][1][text] += 1\n",
    "    \n",
    "    frequencies = []\n",
    "    vocabulary = []\n",
    "    \n",
    "    \n",
    "    # deleting all words that have low frequency\n",
    "    for word in words.keys():\n",
    "        if words[word][0] >= min_freq:\n",
    "            frequencies.append(words[word][1])\n",
    "            vocabulary.append(word)\n",
    "            \n",
    "    bag_of_words_matrix = np.zeros((len(corpus), len(frequencies)), dtype=int)\n",
    "    \n",
    "    for freq_i in range(len(frequencies)):\n",
    "        for text_i in range(len(corpus)):\n",
    "            bag_of_words_matrix[text_i, freq_i] = frequencies[freq_i][text_i]\n",
    "    \n",
    "    return vocabulary, bag_of_words_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca610bc",
   "metadata": {},
   "source": [
    "function for calculating cosine simmilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6e7dce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4733535",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "Generate vectors for both documents and calculate cosine similarity between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3ffe420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': [2, [1, 1]], 'introduced': [7, [7, 0]], 'continuous': [3, [2, 1]], 'skip-gram': [33, [33, 0]], 'model': [75, [33, 42]], 'efficient': [6, [5, 1]], 'method': [7, [7, 0]], 'learning': [14, [9, 5]], 'high-quality': [3, [3, 0]], 'distributed': [3, [3, 0]], 'vector': [23, [23, 0]], 'representations': [45, [37, 8]], 'capture': [1, [1, 0]], 'large': [14, [9, 5]], 'number': [15, [5, 10]], 'precise': [3, [3, 0]], 'syntactic': [4, [3, 1]], 'semantic': [4, [3, 1]], 'word': [44, [44, 0]], 'relationships': [2, [2, 0]], 'paper': [7, [6, 1]], 'extensions': [2, [2, 0]], 'improve': [4, [3, 1]], 'quality': [12, [7, 5]], 'vectors': [28, [26, 2]], 'training': [67, [42, 25]], 'speed': [2, [2, 0]], 'subsampling': [19, [19, 0]], 'frequent': [15, [15, 0]], 'speedup': [3, [3, 0]], 'learn': [9, [4, 5]], 'regular': [1, [1, 0]], 'simple': [14, [12, 2]], 'alternative': [2, [2, 0]], 'hierarchical': [16, [16, 0]], 'softmax': [28, [22, 6]], 'called': [2, [1, 1]], 'negative': [13, [13, 0]], 'sampling': [11, [11, 0]], 'inherent': [1, [1, 0]], 'limitation': [1, [1, 0]], 'indifference': [1, [1, 0]], 'order': [5, [1, 4]], 'inability': [2, [2, 0]], 'represent': [7, [7, 0]], 'idiomatic': [2, [2, 0]], 'phrases': [27, [27, 0]], 'meanings': [3, [3, 0]], 'canada': [2, [2, 0]], 'air': [2, [2, 0]], 'easily': [3, [2, 1]], 'combined': [3, [2, 1]], 'motivated': [1, [1, 0]], 'finding': [2, [2, 0]], 'text': [5, [4, 1]], 'good': [4, [4, 0]], 'millions': [2, [2, 0]], 'introduction': [2, [1, 1]], 'space': [1, [1, 0]], 'algorithms': [1, [1, 0]], 'achieve': [3, [2, 1]], 'performance': [12, [9, 3]], 'natural': [2, [2, 0]], 'language': [10, [7, 3]], 'processing': [2, [1, 1]], 'tasks': [15, [3, 12]], 'grouping': [2, [2, 0]], 'earliest': [1, [1, 0]], 'dates': [1, [1, 0]], 'rumelhart': [1, [1, 0]], 'hinton': [3, [3, 0]], 'williams': [1, [1, 0]], 'idea': [3, [2, 1]], 'applied': [4, [3, 1]], 'statistical': [1, [1, 0]], 'modeling': [8, [3, 5]], 'considerable': [2, [2, 0]], 'success': [1, [1, 0]], 'follow': [1, [1, 0]], 'work': [18, [9, 9]], 'includes': [1, [1, 0]], 'applications': [2, [1, 1]], 'automatic': [1, [1, 0]], 'speech': [1, [1, 0]], 'recognition': [1, [1, 0]], 'machine': [6, [1, 5]], 'translation': [15, [1, 14]], 'wide': [1, [1, 0]], 'range': [2, [2, 0]], 'nlp': [1, [1, 0]], 'mikolov': [4, [4, 0]], 'amounts': [1, [1, 0]], 'unstructured': [1, [1, 0]], 'data': [15, [13, 2]], 'neural': [15, [7, 8]], 'network': [14, [5, 9]], 'architectures': [6, [2, 4]], 'figure': [10, [4, 6]], 'involve': [1, [1, 0]], 'dense': [1, [1, 0]], 'matrix': [5, [1, 4]], 'multiplications': [1, [1, 0]], 'extremely': [3, [2, 1]], 'optimized': [2, [1, 1]], 'single-machine': [1, [1, 0]], 'implementation': [1, [1, 0]], 'train': [7, [3, 4]], 'billion': [6, [6, 0]], 'day': [2, [2, 0]], 'computed': [5, [3, 2]], 'networks': [9, [2, 7]], 'interesting': [3, [3, 0]], 'learned': [15, [9, 6]], 'explicitly': [2, [2, 0]], 'encode': [1, [1, 0]], 'linguistic': [1, [1, 0]], 'regularities': [1, [1, 0]], 'patterns': [2, [2, 0]], 'surprisingly': [3, [2, 1]], 'represented': [2, [1, 1]], 'linear': [13, [7, 6]], 'translations': [2, [1, 1]], 'result': [4, [4, 0]], 'calculation': [1, [1, 0]], 'vec': [21, [21, 0]], 'madrid': [2, [2, 0]], 'spain': [3, [3, 0]], 'france': [6, [6, 0]], 'closer': [1, [1, 0]], 'paris': [4, [4, 0]], 'architecture': [9, [3, 6]], 'objective': [6, [6, 0]], 'predicting': [2, [2, 0]], 'nearby': [1, [1, 0]], 'original': [2, [1, 1]], 'improves': [4, [3, 1]], 'accuracy': [13, [12, 1]], 'addition': [9, [6, 3]], 'simplified': [1, [1, 0]], 'variant': [1, [1, 0]], 'noise': [10, [10, 0]], 'contrastive': [5, [5, 0]], 'estimation': [5, [5, 0]], 'faster': [6, [3, 3]], 'compared': [2, [1, 1]], 'complex': [2, [1, 1]], 'prior': [2, [2, 0]], 'limited': [2, [1, 1]], 'compositions': [1, [1, 0]], 'individual': [4, [3, 1]], 'boston': [4, [4, 0]], 'globe': [2, [2, 0]], 'newspaper': [1, [1, 0]], 'combination': [4, [2, 2]], 'considerably': [3, [2, 1]], 'expressive': [1, [1, 0]], 'techniques': [4, [4, 0]], 'aim': [1, [1, 0]], 'meaning': [2, [2, 0]], 'sentences': [7, [1, 6]], 'composing': [1, [1, 0]], 'recursive': [2, [2, 0]], 'autoencoders': [1, [1, 0]], 'benefit': [2, [1, 1]], 'phrase': [13, [13, 0]], 'extension': [1, [1, 0]], 'based': [12, [7, 5]], 'models': [55, [20, 35]], 'identify': [2, [2, 0]], 'data-driven': [2, [2, 0]], 'approach': [7, [5, 2]], 'treat': [1, [1, 0]], 'tokens': [14, [5, 9]], 'evaluate': [8, [4, 4]], 'developed': [2, [2, 0]], 'test': [4, [4, 0]], 'set': [15, [6, 9]], 'analogical': [9, [9, 0]], 'reasoning': [9, [9, 0]], 'typical': [4, [2, 2]], 'analogy': [5, [5, 0]], 'pair': [1, [1, 0]], 'montreal': [6, [6, 0]], 'canadiens': [3, [3, 0]], 'toronto': [7, [7, 0]], 'maple': [3, [3, 0]], 'leafs': [3, [3, 0]], 'considered': [2, [2, 0]], 'answered': [2, [2, 0]], 'correctly': [2, [2, 0]], 'nearest': [3, [3, 0]], 'representation': [10, [5, 5]], 'finally': [1, [1, 0]], 'property': [4, [3, 1]], 'produce': [2, [1, 1]], 'meaningful': [1, [1, 0]], 'russia': [3, [3, 0]], 'river': [7, [7, 0]], 'close': [3, [3, 0]], 'volga': [4, [4, 0]], 'germany': [4, [4, 0]], 'capital': [6, [6, 0]], 'berlin': [4, [4, 0]], 'compositionality': [3, [3, 0]], 'suggests': [3, [2, 1]], 'non-obvious': [1, [1, 0]], 'degree': [1, [1, 0]], 'understanding': [1, [1, 0]], 'basic': [3, [2, 1]], 'mathematical': [1, [1, 0]], 'operations': [10, [2, 8]], 'surrounding': [2, [2, 0]], 'sentence': [10, [5, 5]], 'document': [1, [1, 0]], 'formally': [1, [1, 0]], 'sequence': [26, [1, 25]], 'maximize': [3, [3, 0]], 'average': [2, [2, 0]], 'log': [12, [12, 0]], 'probability': [7, [7, 0]], 'size': [16, [8, 8]], 'context': [6, [6, 0]], 'function': [17, [3, 14]], 'center': [1, [1, 0]], 'larger': [5, [2, 3]], 'examples': [11, [8, 3]], 'lead': [1, [1, 0]], 'higher': [1, [1, 0]], 'expense': [1, [1, 0]], 'time': [11, [6, 5]], 'formulation': [4, [4, 0]], 'defines': [2, [2, 0]], 'exp': [3, [3, 0]], 'input': [26, [2, 24]], 'output': [32, [4, 28]], 'vocabulary': [9, [5, 4]], 'impractical': [1, [1, 0]], 'cost': [8, [2, 6]], 'computing': [3, [2, 1]], 'proportional': [2, [2, 0]], 'terms': [2, [1, 1]], 'computationally': [2, [2, 0]], 'approximation': [1, [1, 0]], 'morin': [1, [1, 0]], 'bengio': [1, [1, 0]], 'main': [2, [2, 0]], 'advantage': [1, [1, 0]], 'evaluating': [1, [1, 0]], 'nodes': [3, [3, 0]], 'distribution': [6, [6, 0]], 'needed': [1, [1, 0]], 'binary': [3, [3, 0]], 'tree': [6, [6, 0]], 'layer': [25, [2, 23]], 'leaves': [1, [1, 0]], 'node': [4, [4, 0]], 'represents': [1, [1, 0]], 'relative': [3, [1, 2]], 'probabilities': [6, [5, 1]], 'child': [2, [2, 0]], 'define': [2, [2, 0]], 'random': [3, [1, 2]], 'walk': [1, [1, 0]], 'assigns': [3, [3, 0]], 'precisely': [1, [1, 0]], 'reached': [2, [2, 0]], 'path': [8, [3, 5]], 'root': [4, [3, 1]], 'j-th': [1, [1, 0]], 'length': [16, [1, 15]], 'arbitrary': [2, [1, 1]], 'fixed': [3, [1, 2]], 'true': [1, [1, 0]], 'verified': [1, [1, 0]], 'implies': [1, [1, 0]], 'greater': [2, [2, 0]], 'standard': [3, [2, 1]], 'structure': [8, [6, 2]], 'mnih': [4, [4, 0]], 'explored': [1, [1, 0]], 'methods': [1, [1, 0]], 'constructing': [1, [1, 0]], 'huffman': [2, [2, 0]], 'short': [2, [2, 0]], 'codes': [2, [2, 0]], 'fast': [1, [1, 0]], 'observed': [1, [1, 0]], 'frequency': [3, [3, 0]], 'works': [2, [2, 0]], 'technique': [1, [1, 0]], 'gutmann': [1, [1, 0]], 'hyvarinen': [1, [1, 0]], 'teh': [1, [1, 0]], 'nce': [10, [10, 0]], 'posits': [1, [1, 0]], 'differentiate': [1, [1, 0]], 'logistic': [2, [2, 0]], 'regression': [2, [2, 0]], 'hinge': [1, [1, 0]], 'loss': [1, [1, 0]], 'collobert': [3, [3, 0]], 'weston': [2, [2, 0]], 'trained': [18, [9, 9]], 'ranking': [2, [2, 0]], 'concerned': [1, [1, 0]], 'free': [2, [2, 0]], 'simplify': [1, [1, 0]], 'long': [5, [1, 4]], 'retain': [1, [1, 0]], 'country': [2, [2, 0]], 'projected': [3, [1, 2]], 'pca': [2, [2, 0]], 'china': [1, [1, 0]], 'japan': [1, [1, 0]], 'italy': [1, [1, 0]], 'greece': [2, [2, 0]], 'turkey': [1, [1, 0]], 'beijing': [1, [1, 0]], 'tokyo': [1, [1, 0]], 'poland': [1, [1, 0]], 'moscow': [2, [2, 0]], 'portugal': [1, [1, 0]], 'rome': [1, [1, 0]], 'athens': [1, [1, 0]], 'ankara': [1, [1, 0]], 'warsaw': [1, [1, 0]], 'lisbon': [1, [1, 0]], 'two-dimensional': [1, [1, 0]], 'projection': [1, [1, 0]], 'dimensional': [2, [2, 0]], 'countries': [1, [1, 0]], 'cities': [1, [1, 0]], 'illustrates': [1, [1, 0]], 'ability': [2, [1, 1]], 'automatically': [1, [1, 0]], 'organize': [1, [1, 0]], 'concepts': [1, [1, 0]], 'implicitly': [1, [1, 0]], 'provide': [3, [3, 0]], 'supervised': [1, [1, 0]], 'city': [3, [3, 0]], 'replace': [2, [1, 1]], 'term': [1, [1, 0]], 'task': [20, [14, 6]], 'distinguish': [1, [1, 0]], 'target': [2, [1, 1]], 'draws': [1, [1, 0]], 'samples': [4, [4, 0]], 'sample': [3, [3, 0]], 'experiments': [6, [3, 3]], 'values': [20, [2, 18]], 'small': [6, [2, 4]], 'datasets': [2, [2, 0]], 'difference': [2, [2, 0]], 'numerical': [1, [1, 0]], 'maximizes': [1, [1, 0]], 'application': [1, [1, 0]], 'neg': [9, [9, 0]], 'parameter': [2, [1, 1]], 'investigated': [1, [1, 0]], 'choices': [2, [1, 1]], 'unigram': [3, [3, 0]], 'raised': [1, [1, 0]], 'power': [1, [1, 0]], 'outperformed': [1, [1, 0]], 'uniform': [1, [1, 0]], 'distributions': [3, [2, 1]], 'including': [5, [1, 4]], 'reported': [5, [2, 3]], 'corpora': [2, [1, 1]], 'occur': [1, [1, 0]], 'hundreds': [1, [1, 0]], 'times': [6, [5, 1]], 'rare': [4, [4, 0]], 'benefits': [2, [2, 0]], 'observing': [2, [2, 0]], 'co-occurrences': [2, [2, 0]], 'co-occurs': [1, [1, 0]], 'frequently': [3, [3, 0]], 'direction': [1, [1, 0]], 'change': [2, [1, 1]], 'counter': [1, [1, 0]], 'imbalance': [1, [1, 0]], 'discarded': [2, [2, 0]], 'formula': [4, [3, 1]], 'min': [2, [1, 1]], 'total': [4, [1, 3]], 'hs-huffman': [4, [4, 0]], 'table': [27, [13, 14]], 'defined': [1, [1, 0]], 'neg-k': [1, [1, 0]], 'stands': [3, [3, 0]], 'positive': [1, [1, 0]], 'frequency-based': [1, [1, 0]], 'chosen': [4, [3, 1]], 'threshold': [3, [3, 0]], 'typically': [3, [2, 1]], 'chose': [3, [1, 2]], 'aggressively': [1, [1, 0]], 'subsamples': [1, [1, 0]], 'preserving': [1, [1, 0]], 'frequencies': [2, [1, 1]], 'heuristically': [1, [1, 0]], 'practice': [3, [1, 2]], 'accelerates': [1, [1, 0]], 'sections': [2, [1, 1]], 'empirical': [2, [2, 0]], 'consists': [5, [2, 3]], 'analogies': [4, [4, 0]], 'solved': [1, [1, 0]], 'closest': [4, [4, 0]], 'cosine': [2, [1, 1]], 'distance': [3, [1, 2]], 'discard': [1, [1, 0]], 'search': [3, [1, 2]], 'specific': [3, [2, 1]], 'broad': [1, [1, 0]], 'categories': [2, [2, 0]], 'quick': [1, [1, 0]], 'slow': [1, [1, 0]], 'slowly': [1, [1, 0]], 'relationship': [2, [2, 0]], 'dataset': [11, [9, 2]], 'consisting': [4, [2, 2]], 'news': [4, [4, 0]], 'articles': [1, [1, 0]], 'internal': [1, [1, 0]], 'google': [4, [2, 2]], 'occurred': [1, [1, 0]], 'outperforms': [6, [2, 4]], 'accurate': [2, [2, 0]], 'argued': [1, [1, 0]], 'linearity': [1, [1, 0]], 'suitable': [1, [1, 0]], 'sigmoidal': [1, [1, 0]], 'recurrent': [16, [1, 15]], 'highly': [2, [1, 1]], 'non-linear': [2, [2, 0]], 'increases': [1, [1, 0]], 'suggesting': [1, [1, 0]], 'preference': [1, [1, 0]], 'discussed': [1, [1, 0]], 'earlier': [2, [1, 1]], 'composition': [1, [1, 0]], 'infrequently': [1, [1, 0]], 'contexts': [1, [1, 0]], 'york': [3, [3, 0]], 'replaced': [1, [1, 0]], 'unique': [1, [1, 0]], 'bigram': [2, [2, 0]], 'remain': [1, [1, 0]], 'unchanged': [2, [1, 1]], 'code.google.com': [3, [3, 0]], 'source': [3, [2, 1]], 'browse': [2, [2, 0]], 'trunk': [2, [2, 0]], 'questions-words.txt': [1, [1, 0]], 'newspapers': [1, [1, 0]], 'baltimore': [2, [2, 0]], 'sun': [1, [1, 0]], 'jose': [2, [2, 0]], 'mercury': [1, [1, 0]], 'cincinnati': [2, [2, 0]], 'enquirer': [1, [1, 0]], 'nhl': [1, [1, 0]], 'teams': [2, [2, 0]], 'bruins': [1, [1, 0]], 'phoenix': [2, [2, 0]], 'coyotes': [1, [1, 0]], 'nashville': [2, [2, 0]], 'predators': [1, [1, 0]], 'nba': [1, [1, 0]], 'detroit': [2, [2, 0]], 'pistons': [1, [1, 0]], 'raptors': [1, [1, 0]], 'oakland': [1, [1, 0]], 'golden': [1, [1, 0]], 'warriors': [1, [1, 0]], 'memphis': [2, [2, 0]], 'grizzlies': [1, [1, 0]], 'airlines': [5, [5, 0]], 'austria': [1, [1, 0]], 'austrian': [1, [1, 0]], 'spainair': [1, [1, 0]], 'belgium': [1, [1, 0]], 'brussels': [1, [1, 0]], 'aegean': [1, [1, 0]], 'company': [1, [1, 0]], 'executives': [1, [1, 0]], 'steve': [1, [1, 0]], 'ballmer': [1, [1, 0]], 'microsoft': [2, [2, 0]], 'larry': [1, [1, 0]], 'samuel': [1, [1, 0]], 'palmisano': [1, [1, 0]], 'ibm': [1, [1, 0]], 'werner': [1, [1, 0]], 'vogels': [1, [1, 0]], 'amazon': [1, [1, 0]], 'goal': [2, [1, 1]], 'compute': [6, [1, 5]], 'fourth': [1, [1, 0]], 'achieved': [4, [3, 1]], 'form': [2, [1, 1]], 'reasonable': [1, [1, 0]], 'greatly': [2, [1, 1]], 'increasing': [3, [1, 2]], 'theory': [1, [1, 0]], 'n-grams': [1, [1, 0]], 'memory': [5, [1, 4]], 'intensive': [1, [1, 0]], 'scope': [1, [1, 0]], 'compare': [4, [2, 2]], 'decided': [1, [1, 0]], 'formed': [3, [3, 0]], 'counts': [1, [1, 0]], 'score': [6, [2, 4]], 'count': [3, [3, 0]], 'discounting': [1, [1, 0]], 'coefficient': [1, [1, 0]], 'prevents': [1, [1, 0]], 'infrequent': [3, [3, 0]], 'bigrams': [1, [1, 0]], 'passes': [1, [1, 0]], 'decreasing': [2, [1, 1]], 'allowing': [2, [1, 1]], 'longer': [5, [2, 3]], 'involves': [1, [1, 0]], 'publicly': [1, [1, 0]], 'web': [2, [2, 0]], 'starting': [1, [1, 0]], 'previous': [10, [3, 7]], 'constructed': [1, [1, 0]], 'corpus': [2, [2, 0]], 'hyperparameters': [3, [1, 2]], 'dimensionality': [7, [3, 4]], 'setting': [7, [1, 6]], 'achieves': [6, [3, 3]], 'allowed': [1, [1, 0]], 'summarized': [1, [1, 0]], 'respectable': [1, [1, 0]], 'performing': [3, [1, 2]], 'downsampled': [1, [1, 0]], 'cases': [2, [1, 1]], 'questions-phrases.txt': [1, [1, 0]], 'accuracies': [1, [1, 0]], 'vasco': [1, [1, 0]], 'gama': [1, [1, 0]], 'lingsugur': [1, [1, 0]], 'italian': [1, [1, 0]], 'explorer': [1, [1, 0]], 'lake': [1, [1, 0]], 'baikal': [1, [1, 0]], 'great': [2, [2, 0]], 'rift': [1, [1, 0]], 'valley': [1, [1, 0]], 'aral': [1, [1, 0]], 'sea': [2, [2, 0]], 'alan': [1, [1, 0]], 'bean': [1, [1, 0]], 'rebbeca': [1, [1, 0]], 'naomi': [1, [1, 0]], 'moonwalker': [1, [1, 0]], 'ionian': [2, [2, 0]], 'ruegen': [1, [1, 0]], 'islands': [1, [1, 0]], 'chess': [2, [2, 0]], 'master': [1, [1, 0]], 'grandmaster': [1, [1, 0]], 'garry': [1, [1, 0]], 'kasparov': [1, [1, 0]], 'entities': [2, [2, 0]], 'czech': [1, [1, 0]], 'currency': [1, [1, 0]], 'vietnam': [1, [1, 0]], 'german': [1, [1, 0]], 'russian': [2, [2, 0]], 'french': [1, [1, 0]], 'actress': [1, [1, 0]], 'koruna': [1, [1, 0]], 'hanoi': [1, [1, 0]], 'airline': [1, [1, 0]], 'lufthansa': [4, [4, 0]], 'juliette': [1, [1, 0]], 'binoche': [1, [1, 0]], 'check': [1, [1, 0]], 'crown': [1, [1, 0]], 'minh': [1, [1, 0]], 'carrier': [2, [2, 0]], 'vanessa': [1, [1, 0]], 'paradis': [1, [1, 0]], 'polish': [1, [1, 0]], 'zolty': [1, [1, 0]], 'viet': [1, [1, 0]], 'flag': [1, [1, 0]], 'upriver': [1, [1, 0]], 'charlotte': [1, [1, 0]], 'gainsbourg': [1, [1, 0]], 'ctk': [1, [1, 0]], 'vietnamese': [1, [1, 0]], 'cecile': [1, [1, 0]], 'element-wise': [2, [2, 0]], 'sum': [4, [3, 1]], 'increased': [2, [1, 1]], 'entire': [1, [1, 0]], 'reduced': [4, [1, 3]], 'crucial': [2, [2, 0]], 'gain': [1, [1, 0]], 'insight': [2, [2, 0]], 'inspect': [2, [1, 1]], 'manually': [1, [1, 0]], 'neighbours': [2, [2, 0]], 'comparison': [4, [4, 0]], 'consistently': [1, [1, 0]], 'additive': [5, [2, 3]], 'demonstrated': [1, [1, 0]], 'exhibit': [4, [3, 1]], 'perform': [5, [1, 4]], 'arithmetics': [1, [1, 0]], 'interestingly': [2, [2, 0]], 'kind': [1, [1, 0]], 'meaningfully': [2, [2, 0]], 'combine': [1, [1, 0]], 'phenomenon': [1, [1, 0]], 'illustrated': [1, [1, 0]], 'explained': [1, [1, 0]], 'inspecting': [1, [1, 0]], 'inputs': [2, [1, 1]], 'nonlinearity': [1, [1, 0]], 'predict': [1, [1, 0]], 'representing': [1, [1, 0]], 'appears': [2, [2, 0]], 'logarithmically': [2, [1, 1]], 'product': [5, [2, 3]], 'assigned': [2, [1, 1]], 'high': [2, [2, 0]], 'feature': [1, [1, 0]], 'published': [5, [3, 2]], 'authors': [2, [2, 0]], 'worked': [1, [1, 0]], 'turian': [2, [2, 0]], 'downloaded': [1, [1, 0]], 'evaluated': [2, [1, 1]], 'huge': [1, [1, 0]], 'margin': [1, [1, 0]], 'http': [1, [1, 0]], 'metaoptimize.com': [1, [1, 0]], 'projects': [1, [1, 0]], 'wordreprs': [1, [1, 0]], 'redmond': [3, [3, 0]], 'havel': [3, [3, 0]], 'ninjutsu': [1, [1, 0]], 'graffiti': [1, [1, 0]], 'capitulate': [1, [1, 0]], 'conyers': [1, [1, 0]], 'plauen': [1, [1, 0]], 'reiki': [1, [1, 0]], 'cheesecake': [1, [1, 0]], 'abdicate': [1, [1, 0]], 'months': [1, [1, 0]], 'lubbock': [1, [1, 0]], 'dzerzhinsky': [1, [1, 0]], 'kohona': [1, [1, 0]], 'gossip': [1, [1, 0]], 'accede': [1, [1, 0]], 'keene': [1, [1, 0]], 'osterreich': [1, [1, 0]], 'karate': [1, [1, 0]], 'dioramas': [1, [1, 0]], 'rearm': [1, [1, 0]], 'mccarthy': [1, [1, 0]], 'jewell': [1, [1, 0]], 'gunfire': [1, [1, 0]], 'weeks': [1, [1, 0]], 'alston': [1, [1, 0]], 'arzu': [1, [1, 0]], 'emotion': [1, [1, 0]], 'cousins': [1, [1, 0]], 'ovitz': [1, [1, 0]], 'impunity': [1, [1, 0]], 'podhurst': [1, [1, 0]], 'pontiff': [1, [1, 0]], 'anaesthetics': [1, [1, 0]], 'mavericks': [1, [1, 0]], 'days': [5, [1, 4]], 'harlang': [1, [1, 0]], 'pinochet': [1, [1, 0]], 'monkeys': [1, [1, 0]], 'planning': [1, [1, 0]], 'agarwal': [1, [1, 0]], 'rodionov': [1, [1, 0]], 'jews': [1, [1, 0]], 'hesitated': [1, [1, 0]], 'skip-phrase': [1, [1, 0]], 'wash': [1, [1, 0]], 'vaclav': [2, [2, 0]], 'ninja': [1, [1, 0]], 'spray': [1, [1, 0]], 'paint': [1, [1, 0]], 'capitulation': [1, [1, 0]], 'washington': [1, [1, 0]], 'president': [1, [1, 0]], 'martial': [1, [1, 0]], 'arts': [1, [1, 0]], 'grafitti': [1, [1, 0]], 'capitulated': [1, [1, 0]], 'velvet': [1, [1, 0]], 'revolution': [1, [1, 0]], 'swordsmanship': [1, [1, 0]], 'taggers': [1, [1, 0]], 'capitulating': [1, [1, 0]], 'cell': [1, [1, 0]], 'showing': [1, [1, 0]], 'big': [7, [1, 6]], 'visibly': [1, [1, 0]], 'attributed': [1, [1, 0]], 'fact': [2, [1, 1]], 'orders': [2, [2, 0]], 'magnitude': [3, [2, 1]], 'fraction': [4, [1, 3]], 'complexity': [9, [2, 7]], 'required': [3, [1, 2]], 'conclusion': [2, [1, 1]], 'key': [6, [1, 5]], 'contributions': [1, [1, 0]], 'demonstrate': [1, [1, 0]], 'bag-of-words': [1, [1, 0]], 'improvement': [1, [1, 0]], 'uncommon': [1, [1, 0]], 'contribution': [2, [1, 1]], 'algorithm': [3, [2, 1]], 'learns': [2, [1, 1]], 'choice': [2, [2, 0]], 'hyper-parameter': [1, [1, 0]], 'selection': [1, [1, 0]], 'decision': [1, [1, 0]], 'problems': [3, [1, 2]], 'optimal': [1, [1, 0]], 'hyperparameter': [1, [1, 0]], 'configurations': [1, [1, 0]], 'decisions': [1, [1, 0]], 'affect': [1, [1, 0]], 'rate': [5, [1, 4]], 'window': [1, [1, 0]], 'presented': [2, [1, 1]], 'simply': [1, [1, 0]], 'single': [8, [1, 7]], 'token': [1, [1, 0]], 'approaches': [2, [1, 1]], 'powerful': [1, [1, 0]], 'pieces': [1, [1, 0]], 'minimal': [1, [1, 0]], 'computational': [6, [1, 5]], 'complementary': [1, [1, 0]], 'existing': [2, [1, 1]], 'attempts': [1, [1, 0]], 'matrix-vector': [1, [1, 0]], 'code': [3, [1, 2]], 'open-source': [1, [1, 0]], 'project': [2, [1, 1]], 'references': [2, [1, 1]], 'dominant': [1, [0, 1]], 'transduction': [9, [0, 9]], 'convolutional': [8, [0, 8]], 'include': [1, [0, 1]], 'encoder': [19, [0, 19]], 'decoder': [20, [0, 20]], 'connect': [2, [0, 2]], 'attention': [54, [0, 54]], 'mechanism': [5, [0, 5]], 'propose': [2, [0, 2]], 'transformer': [28, [0, 28]], 'solely': [1, [0, 1]], 'mechanisms': [6, [0, 6]], 'dispensing': [1, [0, 1]], 'recurrence': [4, [0, 4]], 'convolutions': [5, [0, 5]], 'superior': [1, [0, 1]], 'parallelizable': [1, [0, 1]], 'requiring': [1, [0, 1]], 'bleu': [11, [0, 11]], 'wmt': [8, [0, 8]], 'englishto-german': [1, [0, 1]], 'improving': [3, [0, 3]], 'ensembles': [4, [0, 4]], 'english-to-french': [5, [0, 5]], 'establishes': [1, [0, 1]], 'single-model': [1, [0, 1]], 'state-of-the-art': [6, [0, 6]], 'gpus': [5, [0, 5]], 'costs': [2, [0, 2]], 'literature': [2, [0, 2]], 'generalizes': [2, [0, 2]], 'applying': [1, [0, 1]], 'english': [4, [0, 4]], 'constituency': [4, [0, 4]], 'parsing': [4, [0, 4]], 'equal': [3, [0, 3]], 'listing': [1, [0, 1]], 'jakob': [1, [0, 1]], 'proposed': [2, [0, 2]], 'replacing': [3, [0, 3]], 'rnns': [2, [0, 2]], 'self-attention': [23, [0, 23]], 'started': [1, [0, 1]], 'effort': [1, [0, 1]], 'ashish': [1, [0, 1]], 'illia': [1, [0, 1]], 'designed': [2, [0, 2]], 'implemented': [4, [0, 4]], 'crucially': [1, [0, 1]], 'involved': [2, [0, 2]], 'aspect': [1, [0, 1]], 'noam': [1, [0, 1]], 'scaled': [6, [0, 6]], 'dot-product': [9, [0, 9]], 'multi-head': [9, [0, 9]], 'parameter-free': [1, [0, 1]], 'position': [12, [0, 12]], 'person': [1, [0, 1]], 'niki': [1, [0, 1]], 'tuned': [1, [0, 1]], 'countless': [2, [0, 2]], 'variants': [2, [0, 2]], 'codebase': [3, [0, 3]], 'tensor': [6, [0, 6]], 'llion': [1, [0, 1]], 'experimented': [2, [0, 2]], 'responsible': [1, [0, 1]], 'initial': [1, [0, 1]], 'inference': [3, [0, 3]], 'visualizations': [1, [0, 1]], 'lukasz': [1, [0, 1]], 'aidan': [1, [0, 1]], 'spent': [1, [0, 1]], 'designing': [1, [0, 1]], 'parts': [1, [0, 1]], 'implementing': [1, [0, 1]], 'massively': [1, [0, 1]], 'accelerating': [1, [0, 1]], 'performed': [4, [0, 4]], 'brain': [1, [0, 1]], 'conference': [1, [0, 1]], 'systems': [1, [0, 1]], 'nips': [1, [0, 1]], 'beach': [1, [0, 1]], 'usa': [1, [0, 1]], 'short-term': [1, [0, 1]], 'gated': [1, [0, 1]], 'firmly': [1, [0, 1]], 'established': [1, [0, 1]], 'art': [3, [0, 3]], 'numerous': [1, [0, 1]], 'efforts': [1, [0, 1]], 'continued': [1, [0, 1]], 'push': [1, [0, 1]], 'boundaries': [1, [0, 1]], 'encoder-decoder': [5, [0, 5]], 'factor': [4, [0, 4]], 'computation': [7, [0, 7]], 'symbol': [3, [0, 3]], 'positions': [22, [0, 22]], 'sequences': [4, [0, 4]], 'aligning': [1, [0, 1]], 'steps': [8, [0, 8]], 'generate': [1, [0, 1]], 'hidden': [5, [0, 5]], 'inherently': [1, [0, 1]], 'sequential': [8, [0, 8]], 'nature': [1, [0, 1]], 'precludes': [1, [0, 1]], 'parallelization': [2, [0, 2]], 'critical': [1, [0, 1]], 'lengths': [3, [0, 3]], 'constraints': [2, [0, 2]], 'limit': [1, [0, 1]], 'batching': [2, [0, 2]], 'improvements': [1, [0, 1]], 'efficiency': [1, [0, 1]], 'factorization': [1, [0, 1]], 'tricks': [1, [0, 1]], 'conditional': [1, [0, 1]], 'case': [5, [0, 5]], 'fundamental': [1, [0, 1]], 'constraint': [1, [0, 1]], 'remains': [1, [0, 1]], 'integral': [1, [0, 1]], 'compelling': [1, [0, 1]], 'dependencies': [7, [0, 7]], 'regard': [1, [0, 1]], 'conjunction': [1, [0, 1]], 'eschewing': [1, [0, 1]], 'relying': [2, [0, 2]], 'draw': [1, [0, 1]], 'global': [1, [0, 1]], 'reach': [1, [0, 1]], 'hours': [2, [0, 2]], 'background': [1, [0, 1]], 'reducing': [2, [0, 2]], 'forms': [1, [0, 1]], 'foundation': [1, [0, 1]], 'extended': [1, [0, 1]], 'gpu': [2, [0, 2]], 'bytenet': [3, [0, 3]], 'convs': [4, [0, 4]], 'building': [1, [0, 1]], 'block': [1, [0, 1]], 'parallel': [4, [0, 4]], 'relate': [1, [0, 1]], 'signals': [2, [0, 2]], 'grows': [1, [0, 1]], 'linearly': [3, [0, 3]], 'difficult': [1, [0, 1]], 'distant': [1, [0, 1]], 'constant': [3, [0, 3]], 'albeit': [1, [0, 1]], 'effective': [1, [0, 1]], 'resolution': [1, [0, 1]], 'averaging': [4, [0, 4]], 'attention-weighted': [1, [0, 1]], 'counteract': [2, [0, 2]], 'intra-attention': [1, [0, 1]], 'relating': [1, [0, 1]], 'variety': [1, [0, 1]], 'reading': [1, [0, 1]], 'comprehension': [1, [0, 1]], 'abstractive': [1, [0, 1]], 'summarization': [1, [0, 1]], 'textual': [1, [0, 1]], 'entailment': [1, [0, 1]], 'task-independent': [1, [0, 1]], 'end-to-end': [1, [0, 1]], 'sequencealigned': [2, [0, 2]], 'simple-language': [1, [0, 1]], 'question': [1, [0, 1]], 'answering': [1, [0, 1]], 'knowledge': [1, [0, 1]], 'convolution': [3, [0, 3]], 'motivate': [1, [0, 1]], 'discuss': [2, [0, 2]], 'advantages': [1, [0, 1]], 'competitive': [2, [0, 2]], 'maps': [1, [0, 1]], 'generates': [1, [0, 1]], 'symbols': [2, [0, 2]], 'element': [1, [0, 1]], 'step': [6, [0, 6]], 'auto-regressive': [2, [0, 2]], 'consuming': [1, [0, 1]], 'generated': [1, [0, 1]], 'additional': [1, [0, 1]], 'generating': [1, [0, 1]], 'stacked': [1, [0, 1]], 'point-wise': [2, [0, 2]], 'fully': [3, [0, 3]], 'connected': [3, [0, 3]], 'layers': [23, [0, 23]], 'left': [1, [0, 1]], 'halves': [1, [0, 1]], 'stacks': [3, [0, 3]], 'composed': [3, [0, 3]], 'stack': [5, [0, 5]], 'identical': [6, [0, 6]], 'sub-layers': [6, [0, 6]], 'positionwise': [1, [0, 1]], 'feed-forward': [5, [0, 5]], 'employ': [4, [0, 4]], 'residual': [5, [0, 5]], 'connection': [1, [0, 1]], 'normalization': [2, [0, 2]], 'sub-layer': [6, [0, 6]], 'layernorm': [1, [0, 1]], 'sublayer': [2, [0, 2]], 'facilitate': [1, [0, 1]], 'connections': [3, [0, 3]], 'embedding': [4, [0, 4]], 'outputs': [4, [0, 4]], 'dimension': [9, [0, 9]], 'dmodel': [11, [0, 11]], 'inserts': [1, [0, 1]], 'performs': [2, [0, 2]], 'modify': [1, [0, 1]], 'prevent': [2, [0, 2]], 'attending': [1, [0, 1]], 'subsequent': [1, [0, 1]], 'masking': [2, [0, 2]], 'embeddings': [8, [0, 8]], 'offset': [2, [0, 2]], 'ensures': [1, [0, 1]], 'predictions': [1, [0, 1]], 'depend': [1, [0, 1]], 'mapping': [2, [0, 2]], 'query': [4, [0, 4]], 'key-value': [1, [0, 1]], 'pairs': [5, [0, 5]], 'weighted': [1, [0, 1]], 'running': [1, [0, 1]], 'weight': [2, [0, 2]], 'compatibility': [4, [0, 4]], 'queries': [7, [0, 7]], 'dot': [7, [0, 7]], 'products': [4, [0, 4]], 'divide': [1, [0, 1]], 'apply': [4, [0, 4]], 'weights': [2, [0, 2]], 'simultaneously': [1, [0, 1]], 'packed': [2, [0, 2]], 'matrices': [2, [0, 2]], 'commonly': [3, [0, 3]], 'functions': [2, [0, 2]], 'multiplicative': [1, [0, 1]], 'scaling': [2, [0, 2]], 'computes': [1, [0, 1]], 'theoretical': [1, [0, 1]], 'space-efficient': [1, [0, 1]], 'multiplication': [1, [0, 1]], 'suspect': [1, [0, 1]], 'grow': [1, [0, 1]], 'pushing': [1, [0, 1]], 'regions': [1, [0, 1]], 'gradients': [1, [0, 1]], 'scale': [1, [0, 1]], 'dmodel-dimensional': [1, [0, 1]], 'beneficial': [2, [0, 2]], 'projections': [2, [0, 2]], 'dimensions': [2, [0, 2]], 'versions': [2, [0, 2]], 'yielding': [2, [0, 2]], 'dv-dimensional': [1, [0, 1]], 'illustrate': [1, [0, 1]], 'assume': [1, [0, 1]], 'components': [2, [0, 2]], 'independent': [1, [0, 1]], 'variables': [1, [0, 1]], 'variance': [2, [0, 2]], 'qiki': [1, [0, 1]], 'concatenated': [1, [0, 1]], 'final': [1, [0, 1]], 'depicted': [1, [0, 1]], 'jointly': [1, [0, 1]], 'attend': [5, [0, 5]], 'subspaces': [1, [0, 1]], 'head': [3, [0, 3]], 'inhibits': [1, [0, 1]], 'multihead': [1, [0, 1]], 'concat': [1, [0, 1]], 'headh': [1, [0, 1]], 'headi': [1, [0, 1]], 'rdmodel': [3, [0, 3]], 'rhdv': [1, [0, 1]], 'heads': [4, [0, 4]], 'single-head': [2, [0, 2]], 'ways': [2, [0, 2]], 'mimics': [1, [0, 1]], 'sequence-to-sequence': [3, [0, 3]], 'place': [1, [0, 1]], 'leftward': [1, [0, 1]], 'flow': [1, [0, 1]], 'preserve': [1, [0, 1]], 'implement': [1, [0, 1]], 'correspond': [1, [0, 1]], 'illegal': [1, [0, 1]], 'position-wise': [1, [0, 1]], 'separately': [1, [0, 1]], 'identically': [1, [0, 1]], 'transformations': [2, [0, 2]], 'relu': [1, [0, 1]], 'activation': [1, [0, 1]], 'max': [1, [0, 1]], 'parameters': [2, [0, 2]], 'describing': [1, [0, 1]], 'kernel': [3, [0, 3]], 'inner-layer': [1, [0, 1]], 'convert': [2, [0, 2]], 'usual': [1, [0, 1]], 'transformation': [2, [0, 2]], 'predicted': [1, [0, 1]], 'next-token': [1, [0, 1]], 'share': [1, [0, 1]], 'pre-softmax': [1, [0, 1]], 'multiply': [1, [0, 1]], 'maximum': [6, [0, 6]], 'per-layer': [1, [0, 1]], 'minimum': [2, [0, 2]], 'types': [3, [0, 3]], 'neighborhood': [2, [0, 2]], 'restricted': [3, [0, 3]], 'type': [1, [0, 1]], 'logk': [2, [0, 2]], 'positional': [10, [0, 10]], 'encoding': [5, [0, 5]], 'inject': [1, [0, 1]], 'absolute': [1, [0, 1]], 'add': [1, [0, 1]], 'encodings': [4, [0, 4]], 'bottoms': [1, [0, 1]], 'summed': [1, [0, 1]], 'sine': [1, [0, 1]], 'sin': [1, [0, 1]], 'cos': [1, [0, 1]], 'corresponds': [2, [0, 2]], 'sinusoid': [1, [0, 1]], 'wavelengths': [1, [0, 1]], 'geometric': [1, [0, 1]], 'progression': [1, [0, 1]], 'hypothesized': [1, [0, 1]], 'pepos': [2, [0, 2]], 'produced': [1, [0, 1]], 'row': [2, [0, 2]], 'sinusoidal': [2, [0, 2]], 'version': [1, [0, 1]], 'extrapolate': [1, [0, 1]], 'encountered': [1, [0, 1]], 'aspects': [1, [0, 1]], 'variable-length': [1, [0, 1]], 'motivating': [1, [0, 1]], 'desiderata': [1, [0, 1]], 'parallelized': [1, [0, 1]], 'measured': [1, [0, 1]], 'long-range': [3, [0, 3]], 'challenge': [1, [0, 1]], 'paths': [3, [0, 3]], 'traverse': [1, [0, 1]], 'shorter': [1, [0, 1]], 'easier': [1, [0, 1]], 'connects': [1, [0, 1]], 'sequentially': [1, [0, 1]], 'executed': [1, [0, 1]], 'requires': [2, [0, 2]], 'smaller': [1, [0, 1]], 'word-piece': [2, [0, 2]], 'byte-pair': [3, [0, 3]], 'involving': [2, [0, 2]], 'centered': [1, [0, 1]], 'respective': [1, [0, 1]], 'increase': [1, [0, 1]], 'plan': [3, [0, 3]], 'investigate': [2, [0, 2]], 'future': [2, [0, 2]], 'width': [1, [0, 1]], 'contiguous': [1, [0, 1]], 'kernels': [1, [0, 1]], 'dilated': [1, [0, 1]], 'longest': [1, [0, 1]], 'generally': [1, [0, 1]], 'expensive': [1, [0, 1]], 'separable': [2, [0, 2]], 'decrease': [1, [0, 1]], 'yield': [1, [0, 1]], 'interpretable': [1, [0, 1]], 'appendix': [1, [0, 1]], 'behavior': [1, [0, 1]], 'describes': [1, [0, 1]], 'regime': [1, [0, 1]], 'english-german': [1, [0, 1]], 'encoded': [1, [0, 1]], 'shared': [1, [0, 1]], 'sourcetarget': [1, [0, 1]], 'english-french': [2, [0, 2]], 'split': [1, [0, 1]], 'batched': [1, [0, 1]], 'approximate': [1, [0, 1]], 'batch': [1, [0, 1]], 'contained': [1, [0, 1]], 'hardware': [1, [0, 1]], 'schedule': [1, [0, 1]], 'nvidia': [1, [0, 1]], 'base': [11, [0, 11]], 'seconds': [2, [0, 2]], 'optimizer': [2, [0, 2]], 'adam': [1, [0, 1]], 'varied': [2, [0, 2]], 'lrate': [1, [0, 1]], 'num': [2, [0, 2]], 'warmup': [3, [0, 3]], 'proportionally': [1, [0, 1]], 'inverse': [1, [0, 1]], 'square': [1, [0, 1]], 'regularization': [2, [0, 2]], 'scores': [1, [0, 1]], 'english-to-german': [6, [0, 6]], 'newstest': [3, [0, 3]], 'tests': [1, [0, 1]], 'en-de': [2, [0, 2]], 'en-fr': [2, [0, 2]], 'deep-att': [2, [0, 2]], 'posunk': [2, [0, 2]], 'gnmt': [2, [0, 2]], 'ensemble': [3, [0, 3]], 'dropout': [6, [0, 6]], 'normalized': [1, [0, 1]], 'sums': [1, [0, 1]], 'pdrop': [3, [0, 3]], 'label': [2, [0, 2]], 'smoothing': [2, [0, 2]], 'employed': [1, [0, 1]], 'hurts': [2, [0, 2]], 'perplexity': [1, [0, 1]], 'unsure': [1, [0, 1]], 'establishing': [1, [0, 1]], 'configuration': [1, [0, 1]], 'listed': [2, [0, 2]], 'surpasses': [1, [0, 1]], 'outperforming': [1, [0, 1]], 'checkpoints': [2, [0, 2]], 'written': [1, [0, 1]], 'minute': [1, [0, 1]], 'intervals': [1, [0, 1]], 'averaged': [1, [0, 1]], 'beam': [5, [0, 5]], 'penalty': [1, [0, 1]], 'experimentation': [1, [0, 1]], 'development': [4, [0, 4]], 'terminate': [1, [0, 1]], 'early': [1, [0, 1]], 'summarizes': [1, [0, 1]], 'compares': [1, [0, 1]], 'estimate': [2, [0, 2]], 'floating': [1, [0, 1]], 'point': [1, [0, 1]], 'multiplying': [1, [0, 1]], 'sustained': [1, [0, 1]], 'single-precision': [1, [0, 1]], 'floating-point': [1, [0, 1]], 'capacity': [1, [0, 1]], 'variations': [2, [0, 2]], 'measuring': [1, [0, 1]], 'tflops': [1, [0, 1]], 'unlisted': [1, [0, 1]], 'metrics': [1, [0, 1]], 'perplexities': [2, [0, 2]], 'per-wordpiece': [1, [0, 1]], 'per-word': [1, [0, 1]], 'params': [1, [0, 1]], 'sinusoids': [1, [0, 1]], 'checkpoint': [1, [0, 1]], 'rows': [3, [0, 3]], 'vary': [1, [0, 1]], 'keeping': [1, [0, 1]], 'worse': [1, [0, 1]], 'drops': [1, [0, 1]], 'observe': [3, [0, 3]], 'determining': [1, [0, 1]], 'easy': [1, [0, 1]], 'sophisticated': [1, [0, 1]], 'expected': [1, [0, 1]], 'bigger': [1, [0, 1]], 'helpful': [1, [0, 1]], 'avoiding': [1, [0, 1]], 'over-fitting': [1, [0, 1]], 'generalize': [1, [0, 1]], 'presents': [1, [0, 1]], 'challenges': [1, [0, 1]], 'subject': [1, [0, 1]], 'strong': [1, [0, 1]], 'structural': [1, [0, 1]], 'rnn': [2, [0, 2]], 'attain': [1, [0, 1]], 'small-data': [1, [0, 1]], 'regimes': [1, [0, 1]], 'wall': [1, [0, 1]], 'street': [1, [0, 1]], 'journal': [1, [0, 1]], 'portion': [1, [0, 1]], 'penn': [1, [0, 1]], 'treebank': [1, [0, 1]], 'semi-supervised': [8, [0, 8]], 'high-confidence': [1, [0, 1]], 'berkleyparser': [1, [0, 1]], 'wsj': [10, [0, 10]], 'select': [1, [0, 1]], 'rates': [1, [0, 1]], 'remained': [1, [0, 1]], 'parser': [1, [0, 1]], 'vinyals': [2, [0, 2]], 'kaiser': [2, [0, 2]], 'discriminative': [5, [0, 5]], 'petrov': [1, [0, 1]], 'dyer': [2, [0, 2]], 'huang': [1, [0, 1]], 'harper': [1, [0, 1]], 'mcclosky': [1, [0, 1]], 'luong': [1, [0, 1]], 'multi-task': [1, [0, 1]], 'generative': [1, [0, 1]], 'lack': [1, [0, 1]], 'task-specific': [1, [0, 1]], 'tuning': [1, [0, 1]], 'exception': [1, [0, 1]], 'grammar': [1, [0, 1]], 'contrast': [1, [0, 1]], 'berkeleyparser': [1, [0, 1]], 'multi-headed': [1, [0, 1]], 'excited': [1, [0, 1]], 'attention-based': [1, [0, 1]], 'extend': [1, [0, 1]], 'modalities': [1, [0, 1]], 'local': [1, [0, 1]], 'efficiently': [1, [0, 1]], 'handle': [1, [0, 1]], 'images': [1, [0, 1]], 'audio': [1, [0, 1]], 'video': [1, [0, 1]], 'making': [1, [0, 1]], 'generation': [1, [0, 1]], 'goals': [1, [0, 1]], 'https': [1, [0, 1]], 'github.com': [1, [0, 1]], 'tensorflow': [1, [0, 1]], 'acknowledgements': [1, [0, 1]], 'grateful': [1, [0, 1]], 'kalchbrenner': [1, [0, 1]], 'stephan': [1, [0, 1]], 'gouws': [1, [0, 1]], 'fruitful': [1, [0, 1]], 'comments': [1, [0, 1]], 'corrections': [1, [0, 1]], 'inspiration': [1, [0, 1]]}\n",
      "0.3266719775975463\n"
     ]
    }
   ],
   "source": [
    "text1_tokens = word_based_tokenization(text1)\n",
    "text2_tokens = word_based_tokenization(text2)\n",
    "corpus = [text1_tokens, text2_tokens]\n",
    "vocabulary, bag_of_words_texts = bag_of_words(corpus)\n",
    "vec_text1 = bag_of_words_texts[0]\n",
    "vec_text2 = bag_of_words_texts[1]\n",
    "\n",
    "print(cos_sim(vec_text1, vec_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b0be4d",
   "metadata": {},
   "source": [
    "# Step 4:\n",
    "Interpret measured cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2e86a53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract: 1, 1 - increased simmilarity\n",
      "introduced: 7, 0 - decreased simmilarity\n",
      "continuous: 2, 1 - increased simmilarity\n",
      "skip-gram: 33, 0 - decreased simmilarity\n",
      "model: 33, 42 - increased simmilarity\n",
      "efficient: 5, 1 - increased simmilarity\n",
      "method: 7, 0 - decreased simmilarity\n",
      "learning: 9, 5 - increased simmilarity\n",
      "high-quality: 3, 0 - decreased simmilarity\n",
      "distributed: 3, 0 - decreased simmilarity\n",
      "vector: 23, 0 - decreased simmilarity\n",
      "representations: 37, 8 - increased simmilarity a little\n",
      "large: 9, 5 - increased simmilarity\n",
      "number: 5, 10 - increased simmilarity\n",
      "precise: 3, 0 - decreased simmilarity\n",
      "syntactic: 3, 1 - increased simmilarity\n",
      "semantic: 3, 1 - increased simmilarity\n",
      "word: 44, 0 - decreased simmilarity\n",
      "relationships: 2, 0 - decreased simmilarity\n",
      "paper: 6, 1 - increased simmilarity\n",
      "extensions: 2, 0 - decreased simmilarity\n",
      "improve: 3, 1 - increased simmilarity\n",
      "quality: 7, 5 - increased simmilarity\n",
      "vectors: 26, 2 - increased simmilarity a little\n",
      "training: 42, 25 - increased simmilarity a little\n",
      "speed: 2, 0 - decreased simmilarity\n",
      "subsampling: 19, 0 - decreased simmilarity\n",
      "frequent: 15, 0 - decreased simmilarity\n",
      "speedup: 3, 0 - decreased simmilarity\n",
      "learn: 4, 5 - increased simmilarity\n",
      "simple: 12, 2 - increased simmilarity a little\n",
      "alternative: 2, 0 - decreased simmilarity\n",
      "hierarchical: 16, 0 - decreased simmilarity\n",
      "softmax: 22, 6 - increased simmilarity a little\n",
      "called: 1, 1 - increased simmilarity\n",
      "negative: 13, 0 - decreased simmilarity\n",
      "sampling: 11, 0 - decreased simmilarity\n",
      "order: 1, 4 - increased simmilarity\n",
      "inability: 2, 0 - decreased simmilarity\n",
      "represent: 7, 0 - decreased simmilarity\n",
      "idiomatic: 2, 0 - decreased simmilarity\n",
      "phrases: 27, 0 - decreased simmilarity\n",
      "meanings: 3, 0 - decreased simmilarity\n",
      "canada: 2, 0 - decreased simmilarity\n",
      "air: 2, 0 - decreased simmilarity\n",
      "easily: 2, 1 - increased simmilarity\n",
      "combined: 2, 1 - increased simmilarity\n",
      "finding: 2, 0 - decreased simmilarity\n",
      "text: 4, 1 - increased simmilarity\n",
      "good: 4, 0 - decreased simmilarity\n",
      "millions: 2, 0 - decreased simmilarity\n",
      "introduction: 1, 1 - increased simmilarity\n",
      "achieve: 2, 1 - increased simmilarity\n",
      "performance: 9, 3 - increased simmilarity\n",
      "natural: 2, 0 - decreased simmilarity\n",
      "language: 7, 3 - increased simmilarity\n",
      "processing: 1, 1 - increased simmilarity\n",
      "tasks: 3, 12 - increased simmilarity\n",
      "grouping: 2, 0 - decreased simmilarity\n",
      "hinton: 3, 0 - decreased simmilarity\n",
      "idea: 2, 1 - increased simmilarity\n",
      "applied: 3, 1 - increased simmilarity\n",
      "modeling: 3, 5 - increased simmilarity\n",
      "considerable: 2, 0 - decreased simmilarity\n",
      "work: 9, 9 - increased simmilarity\n",
      "applications: 1, 1 - increased simmilarity\n",
      "machine: 1, 5 - increased simmilarity\n",
      "translation: 1, 14 - increased simmilarity a little\n",
      "range: 2, 0 - decreased simmilarity\n",
      "mikolov: 4, 0 - decreased simmilarity\n",
      "data: 13, 2 - increased simmilarity a little\n",
      "neural: 7, 8 - increased simmilarity\n",
      "network: 5, 9 - increased simmilarity\n",
      "architectures: 2, 4 - increased simmilarity\n",
      "figure: 4, 6 - increased simmilarity\n",
      "matrix: 1, 4 - increased simmilarity\n",
      "extremely: 2, 1 - increased simmilarity\n",
      "optimized: 1, 1 - increased simmilarity\n",
      "train: 3, 4 - increased simmilarity\n",
      "billion: 6, 0 - decreased simmilarity\n",
      "day: 2, 0 - decreased simmilarity\n",
      "computed: 3, 2 - increased simmilarity\n",
      "networks: 2, 7 - increased simmilarity\n",
      "interesting: 3, 0 - decreased simmilarity\n",
      "learned: 9, 6 - increased simmilarity\n",
      "explicitly: 2, 0 - decreased simmilarity\n",
      "patterns: 2, 0 - decreased simmilarity\n",
      "surprisingly: 2, 1 - increased simmilarity\n",
      "represented: 1, 1 - increased simmilarity\n",
      "linear: 7, 6 - increased simmilarity\n",
      "translations: 1, 1 - increased simmilarity\n",
      "result: 4, 0 - decreased simmilarity\n",
      "vec: 21, 0 - decreased simmilarity\n",
      "madrid: 2, 0 - decreased simmilarity\n",
      "spain: 3, 0 - decreased simmilarity\n",
      "france: 6, 0 - decreased simmilarity\n",
      "paris: 4, 0 - decreased simmilarity\n",
      "architecture: 3, 6 - increased simmilarity\n",
      "objective: 6, 0 - decreased simmilarity\n",
      "predicting: 2, 0 - decreased simmilarity\n",
      "original: 1, 1 - increased simmilarity\n",
      "improves: 3, 1 - increased simmilarity\n",
      "accuracy: 12, 1 - increased simmilarity a little\n",
      "addition: 6, 3 - increased simmilarity\n",
      "noise: 10, 0 - decreased simmilarity\n",
      "contrastive: 5, 0 - decreased simmilarity\n",
      "estimation: 5, 0 - decreased simmilarity\n",
      "faster: 3, 3 - increased simmilarity\n",
      "compared: 1, 1 - increased simmilarity\n",
      "complex: 1, 1 - increased simmilarity\n",
      "prior: 2, 0 - decreased simmilarity\n",
      "limited: 1, 1 - increased simmilarity\n",
      "individual: 3, 1 - increased simmilarity\n",
      "boston: 4, 0 - decreased simmilarity\n",
      "globe: 2, 0 - decreased simmilarity\n",
      "combination: 2, 2 - increased simmilarity\n",
      "considerably: 2, 1 - increased simmilarity\n",
      "techniques: 4, 0 - decreased simmilarity\n",
      "meaning: 2, 0 - decreased simmilarity\n",
      "sentences: 1, 6 - increased simmilarity\n",
      "recursive: 2, 0 - decreased simmilarity\n",
      "benefit: 1, 1 - increased simmilarity\n",
      "phrase: 13, 0 - decreased simmilarity\n",
      "based: 7, 5 - increased simmilarity\n",
      "models: 20, 35 - increased simmilarity a little\n",
      "identify: 2, 0 - decreased simmilarity\n",
      "data-driven: 2, 0 - decreased simmilarity\n",
      "approach: 5, 2 - increased simmilarity\n",
      "tokens: 5, 9 - increased simmilarity\n",
      "evaluate: 4, 4 - increased simmilarity\n",
      "developed: 2, 0 - decreased simmilarity\n",
      "test: 4, 0 - decreased simmilarity\n",
      "set: 6, 9 - increased simmilarity\n",
      "analogical: 9, 0 - decreased simmilarity\n",
      "reasoning: 9, 0 - decreased simmilarity\n",
      "typical: 2, 2 - increased simmilarity\n",
      "analogy: 5, 0 - decreased simmilarity\n",
      "montreal: 6, 0 - decreased simmilarity\n",
      "canadiens: 3, 0 - decreased simmilarity\n",
      "toronto: 7, 0 - decreased simmilarity\n",
      "maple: 3, 0 - decreased simmilarity\n",
      "leafs: 3, 0 - decreased simmilarity\n",
      "considered: 2, 0 - decreased simmilarity\n",
      "answered: 2, 0 - decreased simmilarity\n",
      "correctly: 2, 0 - decreased simmilarity\n",
      "nearest: 3, 0 - decreased simmilarity\n",
      "representation: 5, 5 - increased simmilarity\n",
      "property: 3, 1 - increased simmilarity\n",
      "produce: 1, 1 - increased simmilarity\n",
      "russia: 3, 0 - decreased simmilarity\n",
      "river: 7, 0 - decreased simmilarity\n",
      "close: 3, 0 - decreased simmilarity\n",
      "volga: 4, 0 - decreased simmilarity\n",
      "germany: 4, 0 - decreased simmilarity\n",
      "capital: 6, 0 - decreased simmilarity\n",
      "berlin: 4, 0 - decreased simmilarity\n",
      "compositionality: 3, 0 - decreased simmilarity\n",
      "suggests: 2, 1 - increased simmilarity\n",
      "basic: 2, 1 - increased simmilarity\n",
      "operations: 2, 8 - increased simmilarity\n",
      "surrounding: 2, 0 - decreased simmilarity\n",
      "sentence: 5, 5 - increased simmilarity\n",
      "sequence: 1, 25 - increased simmilarity a little\n",
      "maximize: 3, 0 - decreased simmilarity\n",
      "average: 2, 0 - decreased simmilarity\n",
      "log: 12, 0 - decreased simmilarity\n",
      "probability: 7, 0 - decreased simmilarity\n",
      "size: 8, 8 - increased simmilarity\n",
      "context: 6, 0 - decreased simmilarity\n",
      "function: 3, 14 - increased simmilarity a little\n",
      "larger: 2, 3 - increased simmilarity\n",
      "examples: 8, 3 - increased simmilarity\n",
      "time: 6, 5 - increased simmilarity\n",
      "formulation: 4, 0 - decreased simmilarity\n",
      "defines: 2, 0 - decreased simmilarity\n",
      "exp: 3, 0 - decreased simmilarity\n",
      "input: 2, 24 - increased simmilarity a little\n",
      "output: 4, 28 - increased simmilarity a little\n",
      "vocabulary: 5, 4 - increased simmilarity\n",
      "cost: 2, 6 - increased simmilarity\n",
      "computing: 2, 1 - increased simmilarity\n",
      "proportional: 2, 0 - decreased simmilarity\n",
      "terms: 1, 1 - increased simmilarity\n",
      "computationally: 2, 0 - decreased simmilarity\n",
      "main: 2, 0 - decreased simmilarity\n",
      "nodes: 3, 0 - decreased simmilarity\n",
      "distribution: 6, 0 - decreased simmilarity\n",
      "binary: 3, 0 - decreased simmilarity\n",
      "tree: 6, 0 - decreased simmilarity\n",
      "layer: 2, 23 - increased simmilarity a little\n",
      "node: 4, 0 - decreased simmilarity\n",
      "relative: 1, 2 - increased simmilarity\n",
      "probabilities: 5, 1 - increased simmilarity\n",
      "child: 2, 0 - decreased simmilarity\n",
      "define: 2, 0 - decreased simmilarity\n",
      "random: 1, 2 - increased simmilarity\n",
      "assigns: 3, 0 - decreased simmilarity\n",
      "reached: 2, 0 - decreased simmilarity\n",
      "path: 3, 5 - increased simmilarity\n",
      "root: 3, 1 - increased simmilarity\n",
      "length: 1, 15 - increased simmilarity a little\n",
      "arbitrary: 1, 1 - increased simmilarity\n",
      "fixed: 1, 2 - increased simmilarity\n",
      "greater: 2, 0 - decreased simmilarity\n",
      "standard: 2, 1 - increased simmilarity\n",
      "structure: 6, 2 - increased simmilarity\n",
      "mnih: 4, 0 - decreased simmilarity\n",
      "huffman: 2, 0 - decreased simmilarity\n",
      "short: 2, 0 - decreased simmilarity\n",
      "codes: 2, 0 - decreased simmilarity\n",
      "frequency: 3, 0 - decreased simmilarity\n",
      "works: 2, 0 - decreased simmilarity\n",
      "nce: 10, 0 - decreased simmilarity\n",
      "logistic: 2, 0 - decreased simmilarity\n",
      "regression: 2, 0 - decreased simmilarity\n",
      "collobert: 3, 0 - decreased simmilarity\n",
      "weston: 2, 0 - decreased simmilarity\n",
      "trained: 9, 9 - increased simmilarity\n",
      "ranking: 2, 0 - decreased simmilarity\n",
      "free: 2, 0 - decreased simmilarity\n",
      "long: 1, 4 - increased simmilarity\n",
      "country: 2, 0 - decreased simmilarity\n",
      "projected: 1, 2 - increased simmilarity\n",
      "pca: 2, 0 - decreased simmilarity\n",
      "greece: 2, 0 - decreased simmilarity\n",
      "moscow: 2, 0 - decreased simmilarity\n",
      "dimensional: 2, 0 - decreased simmilarity\n",
      "ability: 1, 1 - increased simmilarity\n",
      "provide: 3, 0 - decreased simmilarity\n",
      "city: 3, 0 - decreased simmilarity\n",
      "replace: 1, 1 - increased simmilarity\n",
      "task: 14, 6 - increased simmilarity\n",
      "target: 1, 1 - increased simmilarity\n",
      "samples: 4, 0 - decreased simmilarity\n",
      "sample: 3, 0 - decreased simmilarity\n",
      "experiments: 3, 3 - increased simmilarity\n",
      "values: 2, 18 - increased simmilarity a little\n",
      "small: 2, 4 - increased simmilarity\n",
      "datasets: 2, 0 - decreased simmilarity\n",
      "difference: 2, 0 - decreased simmilarity\n",
      "neg: 9, 0 - decreased simmilarity\n",
      "parameter: 1, 1 - increased simmilarity\n",
      "choices: 1, 1 - increased simmilarity\n",
      "unigram: 3, 0 - decreased simmilarity\n",
      "distributions: 2, 1 - increased simmilarity\n",
      "including: 1, 4 - increased simmilarity\n",
      "reported: 2, 3 - increased simmilarity\n",
      "corpora: 1, 1 - increased simmilarity\n",
      "times: 5, 1 - increased simmilarity\n",
      "rare: 4, 0 - decreased simmilarity\n",
      "benefits: 2, 0 - decreased simmilarity\n",
      "observing: 2, 0 - decreased simmilarity\n",
      "co-occurrences: 2, 0 - decreased simmilarity\n",
      "frequently: 3, 0 - decreased simmilarity\n",
      "change: 1, 1 - increased simmilarity\n",
      "discarded: 2, 0 - decreased simmilarity\n",
      "formula: 3, 1 - increased simmilarity\n",
      "min: 1, 1 - increased simmilarity\n",
      "total: 1, 3 - increased simmilarity\n",
      "hs-huffman: 4, 0 - decreased simmilarity\n",
      "table: 13, 14 - increased simmilarity\n",
      "stands: 3, 0 - decreased simmilarity\n",
      "chosen: 3, 1 - increased simmilarity\n",
      "threshold: 3, 0 - decreased simmilarity\n",
      "typically: 2, 1 - increased simmilarity\n",
      "chose: 1, 2 - increased simmilarity\n",
      "frequencies: 1, 1 - increased simmilarity\n",
      "practice: 1, 2 - increased simmilarity\n",
      "sections: 1, 1 - increased simmilarity\n",
      "empirical: 2, 0 - decreased simmilarity\n",
      "consists: 2, 3 - increased simmilarity\n",
      "analogies: 4, 0 - decreased simmilarity\n",
      "closest: 4, 0 - decreased simmilarity\n",
      "cosine: 1, 1 - increased simmilarity\n",
      "distance: 1, 2 - increased simmilarity\n",
      "search: 1, 2 - increased simmilarity\n",
      "specific: 2, 1 - increased simmilarity\n",
      "categories: 2, 0 - decreased simmilarity\n",
      "relationship: 2, 0 - decreased simmilarity\n",
      "dataset: 9, 2 - increased simmilarity\n",
      "consisting: 2, 2 - increased simmilarity\n",
      "news: 4, 0 - decreased simmilarity\n",
      "google: 2, 2 - increased simmilarity\n",
      "outperforms: 2, 4 - increased simmilarity\n",
      "accurate: 2, 0 - decreased simmilarity\n",
      "recurrent: 1, 15 - increased simmilarity a little\n",
      "highly: 1, 1 - increased simmilarity\n",
      "non-linear: 2, 0 - decreased simmilarity\n",
      "earlier: 1, 1 - increased simmilarity\n",
      "york: 3, 0 - decreased simmilarity\n",
      "bigram: 2, 0 - decreased simmilarity\n",
      "unchanged: 1, 1 - increased simmilarity\n",
      "code.google.com: 3, 0 - decreased simmilarity\n",
      "source: 2, 1 - increased simmilarity\n",
      "browse: 2, 0 - decreased simmilarity\n",
      "trunk: 2, 0 - decreased simmilarity\n",
      "baltimore: 2, 0 - decreased simmilarity\n",
      "jose: 2, 0 - decreased simmilarity\n",
      "cincinnati: 2, 0 - decreased simmilarity\n",
      "teams: 2, 0 - decreased simmilarity\n",
      "phoenix: 2, 0 - decreased simmilarity\n",
      "nashville: 2, 0 - decreased simmilarity\n",
      "detroit: 2, 0 - decreased simmilarity\n",
      "memphis: 2, 0 - decreased simmilarity\n",
      "airlines: 5, 0 - decreased simmilarity\n",
      "microsoft: 2, 0 - decreased simmilarity\n",
      "goal: 1, 1 - increased simmilarity\n",
      "compute: 1, 5 - increased simmilarity\n",
      "achieved: 3, 1 - increased simmilarity\n",
      "form: 1, 1 - increased simmilarity\n",
      "greatly: 1, 1 - increased simmilarity\n",
      "increasing: 1, 2 - increased simmilarity\n",
      "memory: 1, 4 - increased simmilarity\n",
      "compare: 2, 2 - increased simmilarity\n",
      "formed: 3, 0 - decreased simmilarity\n",
      "score: 2, 4 - increased simmilarity\n",
      "count: 3, 0 - decreased simmilarity\n",
      "infrequent: 3, 0 - decreased simmilarity\n",
      "decreasing: 1, 1 - increased simmilarity\n",
      "allowing: 1, 1 - increased simmilarity\n",
      "longer: 2, 3 - increased simmilarity\n",
      "web: 2, 0 - decreased simmilarity\n",
      "previous: 3, 7 - increased simmilarity\n",
      "corpus: 2, 0 - decreased simmilarity\n",
      "hyperparameters: 1, 2 - increased simmilarity\n",
      "dimensionality: 3, 4 - increased simmilarity\n",
      "setting: 1, 6 - increased simmilarity\n",
      "achieves: 3, 3 - increased simmilarity\n",
      "performing: 1, 2 - increased simmilarity\n",
      "cases: 1, 1 - increased simmilarity\n",
      "great: 2, 0 - decreased simmilarity\n",
      "sea: 2, 0 - decreased simmilarity\n",
      "ionian: 2, 0 - decreased simmilarity\n",
      "chess: 2, 0 - decreased simmilarity\n",
      "entities: 2, 0 - decreased simmilarity\n",
      "russian: 2, 0 - decreased simmilarity\n",
      "lufthansa: 4, 0 - decreased simmilarity\n",
      "carrier: 2, 0 - decreased simmilarity\n",
      "element-wise: 2, 0 - decreased simmilarity\n",
      "sum: 3, 1 - increased simmilarity\n",
      "increased: 1, 1 - increased simmilarity\n",
      "reduced: 1, 3 - increased simmilarity\n",
      "crucial: 2, 0 - decreased simmilarity\n",
      "insight: 2, 0 - decreased simmilarity\n",
      "inspect: 1, 1 - increased simmilarity\n",
      "neighbours: 2, 0 - decreased simmilarity\n",
      "comparison: 4, 0 - decreased simmilarity\n",
      "additive: 2, 3 - increased simmilarity\n",
      "exhibit: 3, 1 - increased simmilarity\n",
      "perform: 1, 4 - increased simmilarity\n",
      "interestingly: 2, 0 - decreased simmilarity\n",
      "meaningfully: 2, 0 - decreased simmilarity\n",
      "inputs: 1, 1 - increased simmilarity\n",
      "appears: 2, 0 - decreased simmilarity\n",
      "logarithmically: 1, 1 - increased simmilarity\n",
      "product: 2, 3 - increased simmilarity\n",
      "assigned: 1, 1 - increased simmilarity\n",
      "high: 2, 0 - decreased simmilarity\n",
      "published: 3, 2 - increased simmilarity\n",
      "authors: 2, 0 - decreased simmilarity\n",
      "turian: 2, 0 - decreased simmilarity\n",
      "evaluated: 1, 1 - increased simmilarity\n",
      "redmond: 3, 0 - decreased simmilarity\n",
      "havel: 3, 0 - decreased simmilarity\n",
      "days: 1, 4 - increased simmilarity\n",
      "vaclav: 2, 0 - decreased simmilarity\n",
      "big: 1, 6 - increased simmilarity\n",
      "fact: 1, 1 - increased simmilarity\n",
      "orders: 2, 0 - decreased simmilarity\n",
      "magnitude: 2, 1 - increased simmilarity\n",
      "fraction: 1, 3 - increased simmilarity\n",
      "complexity: 2, 7 - increased simmilarity\n",
      "required: 1, 2 - increased simmilarity\n",
      "conclusion: 1, 1 - increased simmilarity\n",
      "key: 1, 5 - increased simmilarity\n",
      "contribution: 1, 1 - increased simmilarity\n",
      "algorithm: 2, 1 - increased simmilarity\n",
      "learns: 1, 1 - increased simmilarity\n",
      "choice: 2, 0 - decreased simmilarity\n",
      "problems: 1, 2 - increased simmilarity\n",
      "rate: 1, 4 - increased simmilarity\n",
      "presented: 1, 1 - increased simmilarity\n",
      "single: 1, 7 - increased simmilarity\n",
      "approaches: 1, 1 - increased simmilarity\n",
      "computational: 1, 5 - increased simmilarity\n",
      "existing: 1, 1 - increased simmilarity\n",
      "code: 1, 2 - increased simmilarity\n",
      "project: 1, 1 - increased simmilarity\n",
      "references: 1, 1 - increased simmilarity\n",
      "transduction: 0, 9 - decreased simmilarity\n",
      "convolutional: 0, 8 - decreased simmilarity\n",
      "encoder: 0, 19 - decreased simmilarity\n",
      "decoder: 0, 20 - decreased simmilarity\n",
      "connect: 0, 2 - decreased simmilarity\n",
      "attention: 0, 54 - decreased simmilarity\n",
      "mechanism: 0, 5 - decreased simmilarity\n",
      "propose: 0, 2 - decreased simmilarity\n",
      "transformer: 0, 28 - decreased simmilarity\n",
      "mechanisms: 0, 6 - decreased simmilarity\n",
      "recurrence: 0, 4 - decreased simmilarity\n",
      "convolutions: 0, 5 - decreased simmilarity\n",
      "bleu: 0, 11 - decreased simmilarity\n",
      "wmt: 0, 8 - decreased simmilarity\n",
      "improving: 0, 3 - decreased simmilarity\n",
      "ensembles: 0, 4 - decreased simmilarity\n",
      "english-to-french: 0, 5 - decreased simmilarity\n",
      "state-of-the-art: 0, 6 - decreased simmilarity\n",
      "gpus: 0, 5 - decreased simmilarity\n",
      "costs: 0, 2 - decreased simmilarity\n",
      "literature: 0, 2 - decreased simmilarity\n",
      "generalizes: 0, 2 - decreased simmilarity\n",
      "english: 0, 4 - decreased simmilarity\n",
      "constituency: 0, 4 - decreased simmilarity\n",
      "parsing: 0, 4 - decreased simmilarity\n",
      "equal: 0, 3 - decreased simmilarity\n",
      "proposed: 0, 2 - decreased simmilarity\n",
      "replacing: 0, 3 - decreased simmilarity\n",
      "rnns: 0, 2 - decreased simmilarity\n",
      "self-attention: 0, 23 - decreased simmilarity\n",
      "designed: 0, 2 - decreased simmilarity\n",
      "implemented: 0, 4 - decreased simmilarity\n",
      "involved: 0, 2 - decreased simmilarity\n",
      "scaled: 0, 6 - decreased simmilarity\n",
      "dot-product: 0, 9 - decreased simmilarity\n",
      "multi-head: 0, 9 - decreased simmilarity\n",
      "position: 0, 12 - decreased simmilarity\n",
      "countless: 0, 2 - decreased simmilarity\n",
      "variants: 0, 2 - decreased simmilarity\n",
      "codebase: 0, 3 - decreased simmilarity\n",
      "tensor: 0, 6 - decreased simmilarity\n",
      "experimented: 0, 2 - decreased simmilarity\n",
      "inference: 0, 3 - decreased simmilarity\n",
      "performed: 0, 4 - decreased simmilarity\n",
      "art: 0, 3 - decreased simmilarity\n",
      "encoder-decoder: 0, 5 - decreased simmilarity\n",
      "factor: 0, 4 - decreased simmilarity\n",
      "computation: 0, 7 - decreased simmilarity\n",
      "symbol: 0, 3 - decreased simmilarity\n",
      "positions: 0, 22 - decreased simmilarity\n",
      "sequences: 0, 4 - decreased simmilarity\n",
      "steps: 0, 8 - decreased simmilarity\n",
      "hidden: 0, 5 - decreased simmilarity\n",
      "sequential: 0, 8 - decreased simmilarity\n",
      "parallelization: 0, 2 - decreased simmilarity\n",
      "lengths: 0, 3 - decreased simmilarity\n",
      "constraints: 0, 2 - decreased simmilarity\n",
      "batching: 0, 2 - decreased simmilarity\n",
      "case: 0, 5 - decreased simmilarity\n",
      "dependencies: 0, 7 - decreased simmilarity\n",
      "relying: 0, 2 - decreased simmilarity\n",
      "hours: 0, 2 - decreased simmilarity\n",
      "reducing: 0, 2 - decreased simmilarity\n",
      "gpu: 0, 2 - decreased simmilarity\n",
      "bytenet: 0, 3 - decreased simmilarity\n",
      "convs: 0, 4 - decreased simmilarity\n",
      "parallel: 0, 4 - decreased simmilarity\n",
      "signals: 0, 2 - decreased simmilarity\n",
      "linearly: 0, 3 - decreased simmilarity\n",
      "constant: 0, 3 - decreased simmilarity\n",
      "averaging: 0, 4 - decreased simmilarity\n",
      "counteract: 0, 2 - decreased simmilarity\n",
      "sequencealigned: 0, 2 - decreased simmilarity\n",
      "convolution: 0, 3 - decreased simmilarity\n",
      "discuss: 0, 2 - decreased simmilarity\n",
      "competitive: 0, 2 - decreased simmilarity\n",
      "symbols: 0, 2 - decreased simmilarity\n",
      "step: 0, 6 - decreased simmilarity\n",
      "auto-regressive: 0, 2 - decreased simmilarity\n",
      "point-wise: 0, 2 - decreased simmilarity\n",
      "fully: 0, 3 - decreased simmilarity\n",
      "connected: 0, 3 - decreased simmilarity\n",
      "layers: 0, 23 - decreased simmilarity\n",
      "stacks: 0, 3 - decreased simmilarity\n",
      "composed: 0, 3 - decreased simmilarity\n",
      "stack: 0, 5 - decreased simmilarity\n",
      "identical: 0, 6 - decreased simmilarity\n",
      "sub-layers: 0, 6 - decreased simmilarity\n",
      "feed-forward: 0, 5 - decreased simmilarity\n",
      "employ: 0, 4 - decreased simmilarity\n",
      "residual: 0, 5 - decreased simmilarity\n",
      "normalization: 0, 2 - decreased simmilarity\n",
      "sub-layer: 0, 6 - decreased simmilarity\n",
      "sublayer: 0, 2 - decreased simmilarity\n",
      "connections: 0, 3 - decreased simmilarity\n",
      "embedding: 0, 4 - decreased simmilarity\n",
      "outputs: 0, 4 - decreased simmilarity\n",
      "dimension: 0, 9 - decreased simmilarity\n",
      "dmodel: 0, 11 - decreased simmilarity\n",
      "performs: 0, 2 - decreased simmilarity\n",
      "prevent: 0, 2 - decreased simmilarity\n",
      "masking: 0, 2 - decreased simmilarity\n",
      "embeddings: 0, 8 - decreased simmilarity\n",
      "offset: 0, 2 - decreased simmilarity\n",
      "mapping: 0, 2 - decreased simmilarity\n",
      "query: 0, 4 - decreased simmilarity\n",
      "pairs: 0, 5 - decreased simmilarity\n",
      "weight: 0, 2 - decreased simmilarity\n",
      "compatibility: 0, 4 - decreased simmilarity\n",
      "queries: 0, 7 - decreased simmilarity\n",
      "dot: 0, 7 - decreased simmilarity\n",
      "products: 0, 4 - decreased simmilarity\n",
      "apply: 0, 4 - decreased simmilarity\n",
      "weights: 0, 2 - decreased simmilarity\n",
      "packed: 0, 2 - decreased simmilarity\n",
      "matrices: 0, 2 - decreased simmilarity\n",
      "commonly: 0, 3 - decreased simmilarity\n",
      "functions: 0, 2 - decreased simmilarity\n",
      "scaling: 0, 2 - decreased simmilarity\n",
      "beneficial: 0, 2 - decreased simmilarity\n",
      "projections: 0, 2 - decreased simmilarity\n",
      "dimensions: 0, 2 - decreased simmilarity\n",
      "versions: 0, 2 - decreased simmilarity\n",
      "yielding: 0, 2 - decreased simmilarity\n",
      "components: 0, 2 - decreased simmilarity\n",
      "variance: 0, 2 - decreased simmilarity\n",
      "attend: 0, 5 - decreased simmilarity\n",
      "head: 0, 3 - decreased simmilarity\n",
      "rdmodel: 0, 3 - decreased simmilarity\n",
      "heads: 0, 4 - decreased simmilarity\n",
      "single-head: 0, 2 - decreased simmilarity\n",
      "ways: 0, 2 - decreased simmilarity\n",
      "sequence-to-sequence: 0, 3 - decreased simmilarity\n",
      "transformations: 0, 2 - decreased simmilarity\n",
      "parameters: 0, 2 - decreased simmilarity\n",
      "kernel: 0, 3 - decreased simmilarity\n",
      "convert: 0, 2 - decreased simmilarity\n",
      "transformation: 0, 2 - decreased simmilarity\n",
      "maximum: 0, 6 - decreased simmilarity\n",
      "minimum: 0, 2 - decreased simmilarity\n",
      "types: 0, 3 - decreased simmilarity\n",
      "neighborhood: 0, 2 - decreased simmilarity\n",
      "restricted: 0, 3 - decreased simmilarity\n",
      "logk: 0, 2 - decreased simmilarity\n",
      "positional: 0, 10 - decreased simmilarity\n",
      "encoding: 0, 5 - decreased simmilarity\n",
      "encodings: 0, 4 - decreased simmilarity\n",
      "corresponds: 0, 2 - decreased simmilarity\n",
      "pepos: 0, 2 - decreased simmilarity\n",
      "row: 0, 2 - decreased simmilarity\n",
      "sinusoidal: 0, 2 - decreased simmilarity\n",
      "long-range: 0, 3 - decreased simmilarity\n",
      "paths: 0, 3 - decreased simmilarity\n",
      "requires: 0, 2 - decreased simmilarity\n",
      "word-piece: 0, 2 - decreased simmilarity\n",
      "byte-pair: 0, 3 - decreased simmilarity\n",
      "involving: 0, 2 - decreased simmilarity\n",
      "plan: 0, 3 - decreased simmilarity\n",
      "investigate: 0, 2 - decreased simmilarity\n",
      "future: 0, 2 - decreased simmilarity\n",
      "separable: 0, 2 - decreased simmilarity\n",
      "english-french: 0, 2 - decreased simmilarity\n",
      "base: 0, 11 - decreased simmilarity\n",
      "seconds: 0, 2 - decreased simmilarity\n",
      "optimizer: 0, 2 - decreased simmilarity\n",
      "varied: 0, 2 - decreased simmilarity\n",
      "num: 0, 2 - decreased simmilarity\n",
      "warmup: 0, 3 - decreased simmilarity\n",
      "regularization: 0, 2 - decreased simmilarity\n",
      "english-to-german: 0, 6 - decreased simmilarity\n",
      "newstest: 0, 3 - decreased simmilarity\n",
      "en-de: 0, 2 - decreased simmilarity\n",
      "en-fr: 0, 2 - decreased simmilarity\n",
      "deep-att: 0, 2 - decreased simmilarity\n",
      "posunk: 0, 2 - decreased simmilarity\n",
      "gnmt: 0, 2 - decreased simmilarity\n",
      "ensemble: 0, 3 - decreased simmilarity\n",
      "dropout: 0, 6 - decreased simmilarity\n",
      "pdrop: 0, 3 - decreased simmilarity\n",
      "label: 0, 2 - decreased simmilarity\n",
      "smoothing: 0, 2 - decreased simmilarity\n",
      "hurts: 0, 2 - decreased simmilarity\n",
      "listed: 0, 2 - decreased simmilarity\n",
      "checkpoints: 0, 2 - decreased simmilarity\n",
      "beam: 0, 5 - decreased simmilarity\n",
      "development: 0, 4 - decreased simmilarity\n",
      "estimate: 0, 2 - decreased simmilarity\n",
      "variations: 0, 2 - decreased simmilarity\n",
      "perplexities: 0, 2 - decreased simmilarity\n",
      "rows: 0, 3 - decreased simmilarity\n",
      "observe: 0, 3 - decreased simmilarity\n",
      "rnn: 0, 2 - decreased simmilarity\n",
      "semi-supervised: 0, 8 - decreased simmilarity\n",
      "wsj: 0, 10 - decreased simmilarity\n",
      "vinyals: 0, 2 - decreased simmilarity\n",
      "kaiser: 0, 2 - decreased simmilarity\n",
      "discriminative: 0, 5 - decreased simmilarity\n",
      "dyer: 0, 2 - decreased simmilarity\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bag_of_words_texts[0])):\n",
    "    if (vec_text1[i] == 0 or vec_text2[i] == 0):\n",
    "        print(f\"{vocabulary[i]}: {vec_text1[i]}, {vec_text2[i]} - decreased simmilarity\")\n",
    "    elif (abs(vec_text1[i] - vec_text2[i]) >= 10):\n",
    "        print(f\"{vocabulary[i]}: {vec_text1[i]}, {vec_text2[i]} - increased simmilarity a little\")\n",
    "    else:\n",
    "        print(f\"{vocabulary[i]}: {vec_text1[i]}, {vec_text2[i]} - increased simmilarity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
